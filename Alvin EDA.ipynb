{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import plotly.express as p\n",
    "import warnings\n",
    "import gc\n",
    "import random\n",
    "import numpy as np  \n",
    "random_state = 123\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory reduction helper function:\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:  # columns\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:  # numerics\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(\n",
    "            end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  0.02 Mb (23.5% reduction)\n",
      "Mem. usage decreased to  0.03 Mb (25.9% reduction)\n",
      "Mem. usage decreased to  0.65 Mb (23.6% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (0.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "\n",
    "\n",
    "path_to_dataset = r'C:/Alvin'\n",
    "train = pd.read_csv(path_to_dataset + '/train.csv')\n",
    "test = pd.read_csv(path_to_dataset + '/test.csv')\n",
    "extra_data = pd.read_csv(path_to_dataset + '/extra_data.csv')\n",
    "VariableDefinitions = pd.read_csv(path_to_dataset + '/VariableDefinitions.csv')\n",
    "submission = pd.read_csv(path_to_dataset + '\\SampleSubmission.csv')\n",
    "\n",
    "\n",
    "## reduce memory usage\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)\n",
    "extra_data = reduce_mem_usage(extra_data)\n",
    "VariableDefinitions = reduce_mem_usage(VariableDefinitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERCHANT_CATEGORIZED_AT: The time the merchant was categorized by the customer\n",
      "MERCHANT_NAME: The name of the merchant\n",
      "MERCHANT_CATEGORIZED_AS: The category the merchant was assigned by the customer\n",
      "PURCHASE_VALUE: The value of the purchase made by the customer\n",
      "PURCHASED_AT: The time the purchase was made\n",
      "IS_PURCHASE_PAID_VIA_MPESA_SEND_MONEY: If true indicates that the merchant is not a registered business name\n",
      "USER_EMAIL: The email of the customer\n",
      "USER_AGE: The age of the customer\n",
      "USER_GENDER: The gender of the customer\n",
      "USER_HOUSEHOLD: The number of family members\n"
     ]
    }
   ],
   "source": [
    "### Understand the column names and their meanings\n",
    "\n",
    "for i in range(10):\n",
    "    print(VariableDefinitions.loc[i, 'COLUMN_NAME'] + \":\", VariableDefinitions.loc[i, 'COLUMN_DEFINATION'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (373, 12) test shape:  (558, 11) extra data:  (10000, 12)\n"
     ]
    }
   ],
   "source": [
    "#### Get the size of the train and test dataset\n",
    "\n",
    "print(\"train shape: \",train.shape, \"test shape: \", test.shape, \"extra data: \", extra_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for Train: MERCHANT_CATEGORIZED_AT                    0\n",
      "MERCHANT_NAME                              0\n",
      "MERCHANT_CATEGORIZED_AS                    0\n",
      "PURCHASE_VALUE                             0\n",
      "PURCHASED_AT                               0\n",
      "IS_PURCHASE_PAID_VIA_MPESA_SEND_MONEY      0\n",
      "USER_AGE                                 312\n",
      "USER_GENDER                                6\n",
      "USER_HOUSEHOLD                             0\n",
      "USER_INCOME                                0\n",
      "USER_ID                                    0\n",
      "Transaction_ID                             0\n",
      "dtype: int64\n",
      "\n",
      "Missing values for Test: MERCHANT_CATEGORIZED_AT                    0\n",
      "MERCHANT_NAME                              0\n",
      "PURCHASE_VALUE                             0\n",
      "PURCHASED_AT                               0\n",
      "IS_PURCHASE_PAID_VIA_MPESA_SEND_MONEY      0\n",
      "USER_AGE                                 473\n",
      "USER_GENDER                                5\n",
      "USER_HOUSEHOLD                             0\n",
      "USER_INCOME                                0\n",
      "USER_ID                                    0\n",
      "Transaction_ID                             0\n",
      "dtype: int64\n",
      "\n",
      "Missing values for Extra data: MERCHANT_CATEGORIZED_AT                  10000\n",
      "MERCHANT_NAME                                0\n",
      "MERCHANT_CATEGORIZED_AS                  10000\n",
      "PURCHASE_VALUE                               0\n",
      "PURCHASED_AT                                 0\n",
      "IS_PURCHASE_PAID_VIA_MPESA_SEND_MONEY        0\n",
      "USER_AGE                                  8842\n",
      "USER_GENDER                                 34\n",
      "USER_HOUSEHOLD                               0\n",
      "USER_INCOME                                  0\n",
      "USER_ID                                      0\n",
      "Transaction_ID                               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#### Get the number of missing values in the train and test dataset and extra data\n",
    "print('Missing values for Train:', train.isnull().sum())\n",
    "print('')\n",
    "print('Missing values for Test:', test.isnull().sum())\n",
    "print('')\n",
    "print('Missing values for Extra data:', extra_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique user ids that are both in train and test dataset:  25\n"
     ]
    }
   ],
   "source": [
    "#### To know if the user ids in train dataset are also in test dataset\n",
    "\n",
    "ids_train = list(train.USER_ID.unique())\n",
    "ids_test = list(test.USER_ID.unique())\n",
    "\n",
    "id_train_in_test = [id for id in ids_train if id in ids_test]\n",
    "id_test_in_train = [id for id in ids_test if id in ids_train]\n",
    "\n",
    "print(\"The number of unique user ids that are both in train and test dataset: \",\n",
    "      len(id_train_in_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MERCHANT_CATEGORIZED_AT</th>\n",
       "      <th>MERCHANT_NAME</th>\n",
       "      <th>PURCHASE_VALUE</th>\n",
       "      <th>PURCHASED_AT</th>\n",
       "      <th>IS_PURCHASE_PAID_VIA_MPESA_SEND_MONEY</th>\n",
       "      <th>USER_AGE</th>\n",
       "      <th>USER_GENDER</th>\n",
       "      <th>USER_HOUSEHOLD</th>\n",
       "      <th>USER_INCOME</th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>Transaction_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2022-01-07 09:39:55.446642+00</td>\n",
       "      <td>SAFARICOM LIMITED  ON</td>\n",
       "      <td>10</td>\n",
       "      <td>2022-01-19 14:57:00+00</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>9</td>\n",
       "      <td>100000</td>\n",
       "      <td>ID_1AMEATOU</td>\n",
       "      <td>ID_9ozfeujz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>2022-01-07 09:39:55.446642+00</td>\n",
       "      <td>SAMWEL KIMANI</td>\n",
       "      <td>50</td>\n",
       "      <td>2022-01-20 04:06:00+00</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>9</td>\n",
       "      <td>100000</td>\n",
       "      <td>ID_1AMEATOU</td>\n",
       "      <td>ID_c7v0jvvs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2022-01-07 09:39:55.446642+00</td>\n",
       "      <td>BUY DIRECT LIMITED 2</td>\n",
       "      <td>10386</td>\n",
       "      <td>2021-12-02 14:19:00+00</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>9</td>\n",
       "      <td>100000</td>\n",
       "      <td>ID_1AMEATOU</td>\n",
       "      <td>ID_gxj2nky1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>2022-04-12 15:59:14.139347+00</td>\n",
       "      <td>ALLSTAR GAS SUPPLIES</td>\n",
       "      <td>40</td>\n",
       "      <td>2022-04-25 18:01:00+00</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>8</td>\n",
       "      <td>30000</td>\n",
       "      <td>ID_QIBWKFP0</td>\n",
       "      <td>ID_kws09f8o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>2021-08-06 11:28:03.289361+00</td>\n",
       "      <td>ALEX  SAKWA</td>\n",
       "      <td>100</td>\n",
       "      <td>2021-08-13 09:20:00+00</td>\n",
       "      <td>True</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>200000</td>\n",
       "      <td>ID_9OCPCLOX</td>\n",
       "      <td>ID_qmxqh91v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>2022-05-31 13:27:24.704992+00</td>\n",
       "      <td>SAMWEL KIMANI</td>\n",
       "      <td>257</td>\n",
       "      <td>2022-05-29 14:22:00+00</td>\n",
       "      <td>True</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>5</td>\n",
       "      <td>80000</td>\n",
       "      <td>ID_KVE1YQI4</td>\n",
       "      <td>ID_u24yokj0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>2022-05-04 18:39:01.19219+00</td>\n",
       "      <td>DORINE ADHIAMBO</td>\n",
       "      <td>2000</td>\n",
       "      <td>2022-05-11 17:33:00+00</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>4</td>\n",
       "      <td>36000</td>\n",
       "      <td>ID_90M21S6P</td>\n",
       "      <td>ID_urkqqvp5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           MERCHANT_CATEGORIZED_AT          MERCHANT_NAME  PURCHASE_VALUE  \\\n",
       "142  2022-01-07 09:39:55.446642+00  SAFARICOM LIMITED  ON              10   \n",
       "185  2022-01-07 09:39:55.446642+00          SAMWEL KIMANI              50   \n",
       "246  2022-01-07 09:39:55.446642+00   BUY DIRECT LIMITED 2           10386   \n",
       "316  2022-04-12 15:59:14.139347+00   ALLSTAR GAS SUPPLIES              40   \n",
       "400  2021-08-06 11:28:03.289361+00            ALEX  SAKWA             100   \n",
       "460  2022-05-31 13:27:24.704992+00          SAMWEL KIMANI             257   \n",
       "468   2022-05-04 18:39:01.19219+00        DORINE ADHIAMBO            2000   \n",
       "\n",
       "               PURCHASED_AT  IS_PURCHASE_PAID_VIA_MPESA_SEND_MONEY  USER_AGE  \\\n",
       "142  2022-01-19 14:57:00+00                                  False       NaN   \n",
       "185  2022-01-20 04:06:00+00                                   True       NaN   \n",
       "246  2021-12-02 14:19:00+00                                  False       NaN   \n",
       "316  2022-04-25 18:01:00+00                                  False       NaN   \n",
       "400  2021-08-13 09:20:00+00                                   True      29.0   \n",
       "460  2022-05-29 14:22:00+00                                   True      32.0   \n",
       "468  2022-05-11 17:33:00+00                                   True       NaN   \n",
       "\n",
       "    USER_GENDER  USER_HOUSEHOLD  USER_INCOME      USER_ID Transaction_ID  \n",
       "142      Female               9       100000  ID_1AMEATOU    ID_9ozfeujz  \n",
       "185      Female               9       100000  ID_1AMEATOU    ID_c7v0jvvs  \n",
       "246      Female               9       100000  ID_1AMEATOU    ID_gxj2nky1  \n",
       "316        Male               8        30000  ID_QIBWKFP0    ID_kws09f8o  \n",
       "400        Male               1       200000  ID_9OCPCLOX    ID_qmxqh91v  \n",
       "460        Male               5        80000  ID_KVE1YQI4    ID_u24yokj0  \n",
       "468      Female               4        36000  ID_90M21S6P    ID_urkqqvp5  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### The unique user ids in test dataset that are not present within train\n",
    "\n",
    "ids_remaining = [id for id in ids_test if id not in ids_train]\n",
    "test.loc[test['USER_ID'].isin(ids_remaining)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To avoid data leakage you will sort the Dataframe based on USER_ID and PURCHASED AT (which is the time the unique user did a purchase).  When setting your Validation and Test dataset, you will use USER_IDs that the model hasn't seen before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values(by=['USER_ID', 'PURCHASED_AT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bills & Fees        78\n",
       "Groceries           50\n",
       "Miscellaneous       43\n",
       "Data & WiFi         43\n",
       "Going out           41\n",
       "Family & Friends    41\n",
       "Transport & Fuel    29\n",
       "Shopping            21\n",
       "Emergency fund      12\n",
       "Health               6\n",
       "Loan Repayment       5\n",
       "Education            2\n",
       "Rent / Mortgage      2\n",
       "Name: MERCHANT_CATEGORIZED_AS, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### How balanced is the target variable for the dataset\n",
    "\n",
    "train.MERCHANT_CATEGORIZED_AS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique size of merchants in train dataset: 218\n",
      "unique size of merchants in extra dataset: 2781\n",
      "unique size of merchants in test dataset: 314\n",
      "merchants names in extra dataset that are also in train dataset:  91\n",
      "merchant_extra_data shape:  (3243, 12)\n",
      "merchants names in extra dataset that are also in test dataset:  126\n",
      "merchant_extra_data shape:  (3656, 12)\n"
     ]
    }
   ],
   "source": [
    "### Since we would be using the extra dataset we need to know the  unique MERCHANT_NAME in the extra dataset that are also in the train dataset\n",
    "\n",
    "merchants_train = list(train.MERCHANT_NAME.unique())\n",
    "merchants_extra = list(extra_data.MERCHANT_NAME.unique())\n",
    "merchants_test = list(test.MERCHANT_NAME.unique())\n",
    "\n",
    "\n",
    "print(\"unique size of merchants in train dataset:\", len(merchants_train))\n",
    "print(\"unique size of merchants in extra dataset:\", len(merchants_extra))\n",
    "print(\"unique size of merchants in test dataset:\", len(merchants_test))\n",
    "\n",
    "\n",
    "### Merchants name in extra dataset that are also in train dataset \n",
    "\n",
    "merchant_extra_in_train = [merchant for merchant in merchants_extra if merchant in merchants_train]\n",
    "print(\"merchants names in extra dataset that are also in train dataset: \", len(merchant_extra_in_train))\n",
    "\n",
    "##### Get the rows of the merchants name that are in the extra dataset and are also in the train dataset\n",
    "\n",
    "merchant_extra_data_train = extra_data.loc[extra_data['MERCHANT_NAME'].isin(merchant_extra_in_train)]\n",
    "print(\"merchant_extra_data shape: \", merchant_extra_data_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "### Merchants name in extra dataset that are also in test dataset\n",
    "\n",
    "merchant_extra_in_test = [\n",
    "    merchant for merchant in merchants_extra if merchant in merchants_test]\n",
    "print(\"merchants names in extra dataset that are also in test dataset: \",\n",
    "      len(merchant_extra_in_test))\n",
    "\n",
    "# ##### Get the rows of the merchants name that are in the extra dataset and are also in the train dataset\n",
    "\n",
    "merchant_extra_data_test = extra_data.loc[extra_data['MERCHANT_NAME'].isin(\n",
    "    merchant_extra_in_test)]\n",
    "print(\"merchant_extra_data shape: \", merchant_extra_data_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Get the merchant names in the merchant_extra_in_train list from the train dataset \n",
    "\n",
    "merchant_train_data = train.loc[train['MERCHANT_NAME'].isin(\n",
    "    merchant_extra_in_train)]\n",
    "\n",
    "\n",
    "##### put the merchant name and the merchant category into two lists and convert them into a single dictionary\n",
    "\n",
    "list_merchants_name = list(train.MERCHANT_NAME.values) \n",
    "list_merchants_category = list(train.MERCHANT_CATEGORIZED_AS.values)\n",
    "dict_merchants = dict(zip(list_merchants_name, list_merchants_category))\n",
    "\n",
    "###### Map their values unto the MERCHANT_CATEGORIZED_AS column in the merchant_extra_data\n",
    "\n",
    "merchant_extra_data_train['MERCHANT_CATEGORIZED_AS'] = merchant_extra_data_train['MERCHANT_NAME'].map(\n",
    "    dict_merchants)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Merchant Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POA', 'QUICK MART', 'NAIVAS SUPERMARKET', 'SAFARICOM LIMITED', 'NAIVAS KITENGELA', 'NAIVAS', 'NAIROBI JAVA', 'SAFARICOM POSTPAID', 'PAYTECH LIMITED', 'KAPS PARKING', 'RUBIS', 'PESAPAL', 'NAIROBI JAVA HOUSE SARIT CENTRE', 'JAVA', 'CARREFOUR SRT', 'SAFARICOM POST', 'SAFARICOM POSTPAID BUNDLES', 'FAMILY BANK PESA PAP', 'EQUITY PAYBILL ACCOUNT', 'NABO CAPITAL LTD C2B', 'M-SHWARI ACCOUNT', 'SAFARICOM OFFERS  TUNUKIWA', 'IPAY LTD', 'ASL TD', 'IM BANK C2B', 'EQUITY PAYBILL', 'STATES BARBERSHOP', 'OAKS  CORKS - ONLINE', 'SHELL ST AUSTINS SERVICES STATION', 'KRA - NAIROBI COUNTY REVENUE', 'PAYTECH -THE JUNCTION MALL', 'SHOP AND', 'ZUKU  FOR', 'OILIBYA WAIYAKI', 'DR CECILIA', 'KENGELES-GARDEN BAR', 'ALCHEMIIST - LEGEND BAR', 'THE SPOT POOL BAR', 'SAFARICOM HOME', 'FISH BAY', 'PESAPAL  FOR', 'TUSKYS -T MALL', 'SEVEN EIGHT SIX BUTCHERY', 'THREE BINS SERVICES', 'CAPTON ENTERPRISES NRBI WEST', 'TOTAL K STATE HOUSE 1', 'GALITOS BELLEVUE', 'CAFETERIA AND TIN TIN RESTAURANT', 'FARMERS BUTCHERY', 'METROMART LTD', 'GITHUNGURI SELFRIDGES SUPERMARKET', 'GOODLIFE PHARMACY -SARIT CENTRE 2', 'HUHUS PIZZA LIMITED HO', 'NHIF', 'QUICK MART FRESH N EASY BYPASS', 'OUTSKIRTS PLACE', 'MBAIRES RESTAURANT 6', 'SAFARICOM POST PAID', 'APS ABC PARKING', 'JAVA AGAKHAN MAIN', 'GARIDON ENTERPRISES VIA CO-OP', 'PETROCITY ENTERPRISES LIMITED', 'KEMAHMWA ENTERPRISE HQ', 'FATUMA OSORE', 'MICHAEL MUSEMBI', 'HARRISON JUMA AYUAK', 'FLORENCE KANANA', 'NAIVAS LIFESTYLE', 'UONSDA CHURCH  MOGERE MWAYO TITHE', 'VITAMIX FRESH FRUITS AND JUICES', 'KILIMANJARO LUNCHEONETTE', 'JIWEKE TAVERN', 'TUSKYS -CHAP CHAP THIGIRI', 'CRAFT - CHUPA CHAP', 'NAIROBI WATER  SEWERAGE CO LTD', 'ELIVIN KWAMBOKA MOYWAYWA', 'MOGO AUTO LIMITED  KDA', 'KILELESHWA SUPERSHINE', 'TOTAL ENERGIES KILELESHWA-LEMPIRAS LIMITED', 'OWETTE DISHES', 'ART MARKET KILELESHWA', 'TOTAL - KIAMBU ROAD', 'NIFTY TIGONI LIMITED', 'TOTALENERGIES KITISURU', 'SKY-GARDEN', 'BAABAX EXPRESS LIMITED', 'VINCENT KARIUKI MBURU', 'NAIVAS AGA KHAN WALK', 'REPUBLIQOUR HUB', 'KUNE FOOD LAB KENYA LIMITED', 'ONAIRES CATERERS']\n"
     ]
    }
   ],
   "source": [
    "### Print all unique merchant names in train dataset\n",
    "\n",
    "print(merchant_extra_data_train.MERCHANT_NAME.unique().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Move user_id as the first column\n",
    "\n",
    "first_column = train.pop(\"USER_ID\")\n",
    "train.insert(0, 'USER_ID', first_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KAPS PARKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID']== \"ID_3JA0MAFB\")\\\n",
    "     & (merchant_extra_data_train['MERCHANT_NAME']== \"KAPS PARKING\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Transport & Fuel\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_ECX9BS4A\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"KAPS PARKING\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Going out\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_O8P8YS18\")\\\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"KAPS PARKING\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Bills & Fees\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_Y0386AT9\")\n",
    "     & (merchant_extra_data_train['MERCHANT_NAME']== \"KAPS PARKING\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Miscellaneous\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAFARICOM LIMITED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_ECX9BS4A\")\n",
    "     & (merchant_extra_data_train['MERCHANT_NAME']== \"SAFARICOM LIMITED\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Data & WiFi\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_GAQ3PX9G\")\n",
    "     & (merchant_extra_data_train['MERCHANT_NAME']== \"SAFARICOM LIMITED\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Health\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_GR569FUO\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"SAFARICOM LIMITED\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Miscellaneous\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_OZANC6XT\")\n",
    "     & (merchant_extra_data_train['MERCHANT_NAME']== \"SAFARICOM LIMITED\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Data & WiFi\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_ZX4DCF4K\")\n",
    "     & (merchant_extra_data_train['MERCHANT_NAME']== \"SAFARICOM LIMITED\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Data & WiFi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIVAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_ECX9BS4A\")\n",
    "     & (merchant_extra_data_train['MERCHANT_NAME']== \"NAIVAS\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Groceries\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_U9WZMGJZ\")\n",
    "     & (merchant_extra_data_train['MERCHANT_NAME']== \"NAIVAS\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Family & Friends\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CARREFOUR SRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_ECX9BS4A\")\n",
    "     & (merchant_extra_data_train['MERCHANT_NAME']== \"CARREFOUR SRT\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Groceries\"\n",
    "\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_O8P8YS18\")\n",
    "     & (merchant_extra_data_train['MERCHANT_NAME']== \"CARREFOUR SRT\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Shopping\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EQUITY PAYBILL ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_L8QMYB09\")\n",
    "     & (merchant_extra_data_train['MERCHANT_NAME']== \"EQUITY PAYBILL ACCOUNT\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Shopping\"\n",
    "\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_U9WZMGJZ\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"EQUITY PAYBILL ACCOUNT\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Bills & Fees\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_Y0386AT9\")\n",
    "     & (merchant_extra_data_train['MERCHANT_NAME']== \"EQUITY PAYBILL ACCOUNT\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Groceries\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M-SHWARI ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_1I8XYBWK\")\n",
    "     & (merchant_extra_data_train['MERCHANT_NAME']== \"M-SHWARI ACCOUNT\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Emergency fund\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_J8O7LHZ2\")\n",
    "     & (merchant_extra_data_train['MERCHANT_NAME']== \"M-SHWARI ACCOUNT\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Emergency fund\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_KARF7R4R\")\n",
    "     & (merchant_extra_data_train['MERCHANT_NAME']== \"M-SHWARI ACCOUNT\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Emergency fund\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_UJ0YSYEV\")\n",
    "     & (merchant_extra_data_train['MERCHANT_NAME']== \"M-SHWARI ACCOUNT\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Emergency fund\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_ZX4DCF4K\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"M-SHWARI ACCOUNT\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Bills & Fees\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAFARICOM OFFERS  TUNUKIWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_GR569FUO\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"SAFARICOM OFFERS  TUNUKIWA\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Miscellaneous\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IM BANK C2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_180RJKP4\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"IM BANK C2B\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Bills & Fees\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_ECX9BS4A\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"IM BANK C2B\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Rent / Mortgage\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOTAL K STATE HOUSE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_1I8XYBWK\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"TOTAL K STATE HOUSE 1\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Transport & Fuel\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FARMERS BUTCHERY\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_O8P8YS18\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"FARMERS BUTCHERY\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Groceries\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NHIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_40L9OTIM\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"NHIF\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Bills & Fees\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_Y0386AT9\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"NHIF\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Health\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAFARICOM POST PAID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_3JA0MAFB\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"SAFARICOM POST PAID\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Data & WiFi\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_D8FOVVBB\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"SAFARICOM POST PAID\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Bills & Fees\"\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_U9WZMGJZ\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"SAFARICOM POST PAID\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Data & WiFi\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIVAS LIFESTYLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_GR569FUO\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"NAIVAS LIFESTYLE\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Shopping\"\n",
    "\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_ZX4DCF4K\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"NAIVAS LIFESTYLE\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Bills & Fees\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KILELESHWA SUPERSHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_3JA0MAFB\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"KILELESHWA SUPERSHINE\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Miscellaneous\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KUNE FOOD LAB KENYA LIMITED\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_3JA0MAFB\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"KUNE FOOD LAB KENYA LIMITED\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Going out\"\n",
    "\n",
    "\n",
    "merchant_extra_data_train.loc[(merchant_extra_data_train['USER_ID'] == \"ID_D8FOVVBB\")\n",
    "                              & (merchant_extra_data_train['MERCHANT_NAME'] == \"KUNE FOOD LAB KENYA LIMITED\"), \"MERCHANT_CATEGORIZED_AS\"] = \"Miscellaneous\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a column to signify that the target variable is imputted \n",
    "\n",
    "merchant_extra_data_train['generated_target'] = 0\n",
    "train['generated_target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(extra_data.MERCHANT_NAME.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of train_df : (3616, 13)\n"
     ]
    }
   ],
   "source": [
    "## merge the train and the merchant_extra_data together\n",
    "\n",
    "train_df = pd.concat([train, merchant_extra_data_train], axis=0)\n",
    "print(\"the size of train_df :\", train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the USER DATE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USER_ID                                     0\n",
       "MERCHANT_CATEGORIZED_AT                  3243\n",
       "MERCHANT_NAME                               0\n",
       "MERCHANT_CATEGORIZED_AS                     0\n",
       "PURCHASE_VALUE                              0\n",
       "PURCHASED_AT                                0\n",
       "IS_PURCHASE_PAID_VIA_MPESA_SEND_MONEY       0\n",
       "USER_AGE                                 3293\n",
       "USER_GENDER                                13\n",
       "USER_HOUSEHOLD                              0\n",
       "USER_INCOME                                 0\n",
       "Transaction_ID                              0\n",
       "generated_target                            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MERCHANT_CATEGORIZED_AT                    0\n",
       "MERCHANT_NAME                              0\n",
       "PURCHASE_VALUE                             0\n",
       "PURCHASED_AT                               0\n",
       "IS_PURCHASE_PAID_VIA_MPESA_SEND_MONEY      0\n",
       "USER_AGE                                 473\n",
       "USER_GENDER                                5\n",
       "USER_HOUSEHOLD                             0\n",
       "USER_INCOME                                0\n",
       "USER_ID                                    0\n",
       "Transaction_ID                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values for Extra data: MERCHANT_CATEGORIZED_AT                  3243\n",
      "MERCHANT_NAME                               0\n",
      "MERCHANT_CATEGORIZED_AS                     0\n",
      "PURCHASE_VALUE                              0\n",
      "PURCHASED_AT                                0\n",
      "IS_PURCHASE_PAID_VIA_MPESA_SEND_MONEY       0\n",
      "USER_AGE                                 2981\n",
      "USER_GENDER                                 7\n",
      "USER_HOUSEHOLD                              0\n",
      "USER_INCOME                                 0\n",
      "USER_ID                                     0\n",
      "Transaction_ID                              0\n",
      "generated_target                            0\n",
      "dtype: int64\n",
      "\n",
      "we have only 323 of true data to predict the age column\n"
     ]
    }
   ],
   "source": [
    "#####  Get to look at the number of missing values for ages in merchant_extra_data\n",
    "\n",
    "print('')\n",
    "print('Missing values for Extra data:', merchant_extra_data_train.isnull().sum())\n",
    "print('')\n",
    "\n",
    "print(\"we have only {} of true data to predict the age column\".format(\n",
    "train_df.shape[0] - (2981+312)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Dataframe to predict age is now:  (323, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "USER_ID                                  0\n",
       "MERCHANT_NAME                            0\n",
       "MERCHANT_CATEGORIZED_AS                  0\n",
       "PURCHASE_VALUE                           0\n",
       "PURCHASED_AT                             0\n",
       "IS_PURCHASE_PAID_VIA_MPESA_SEND_MONEY    0\n",
       "USER_AGE                                 0\n",
       "USER_GENDER                              0\n",
       "USER_HOUSEHOLD                           0\n",
       "USER_INCOME                              0\n",
       "Transaction_ID                           0\n",
       "generated_target                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Delete merchant categorized at and remove rows with nan values\n",
    "\n",
    "train_df_age = train_df.copy() ##### create a copy of train_df\n",
    "\n",
    "del train_df_age['MERCHANT_CATEGORIZED_AT']\n",
    "train_df_age.dropna(axis=0, how = 'any', inplace=True)\n",
    "\n",
    "print(\"Size of Dataframe to predict age is now: \", train_df_age.shape)\n",
    "\n",
    "train_df_age.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model to predict the age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID_CVK8ERW1' 'ID_L8QMYB09' 'ID_U9WZMGJZ' 'ID_WO19RXAS' 'ID_KVE1YQI4'\n",
      " 'ID_FBK8QIYB']\n"
     ]
    }
   ],
   "source": [
    "print(train_df_age.USER_ID.unique())\n",
    "\n",
    "#### Sort the dataframe by User ID and date\n",
    "\n",
    "train_df_age.sort_values(by=['USER_ID', 'USER_AGE'], inplace=True)\n",
    "train_df_age.reset_index(inplace=True)\n",
    "del train_df_age['index']\n",
    "\n",
    "train_ids = [\"ID_WO19RXAS\",\n",
    "             \"ID_FBK8QIYB\",\n",
    "             \"ID_CVK8ERW1\",\n",
    "             \"ID_KVE1YQI4\"]\n",
    "\n",
    "test_ids = [\"ID_U9WZMGJZ\", \"ID_L8QMYB09\"]\n",
    "\n",
    "##### Use different USER_ID for train and validation set\n",
    "\n",
    "train_dataset = train_df_age.loc[train_df_age['USER_ID'].isin(train_ids)]\n",
    "train_dataset.reset_index(inplace=True)\n",
    "del train_dataset['index']\n",
    "test_dataset = train_df_age.loc[train_df_age['USER_ID'].isin(test_ids)]\n",
    "test_dataset.reset_index(inplace=True)\n",
    "del test_dataset['index']\n",
    "\n",
    "#### Specify the columns to be used for the training and testing\n",
    "\n",
    "unuseful_columns = ['MERCHANT_NAME', 'PURCHASED_AT', 'MERCHANT_CATEGORIZED_AS', 'USER_GENDER',\n",
    "                    'Transaction_ID', 'generated_target', 'USER_AGE', 'USER_ID', 'IS_PURCHASE_PAID_VIA_MPESA_SEND_MONEY',\n",
    "                    'USER_GENDER',\n",
    "                    'USER_HOUSEHOLD',\n",
    "                    'USER_INCOME']\n",
    "features = [\n",
    "    feature for feature in train_df_age.columns if feature not in unuseful_columns]\n",
    "features\n",
    "\n",
    "\n",
    "#### Change the data type of the features to int and category\n",
    "\n",
    "# categories = []\n",
    "# for col in categories:\n",
    "#     train_dataset[col] = train_dataset[col].astype(\"category\")\n",
    "#     test_dataset[col] = test_dataset[col].astype(\"category\")\n",
    "#     train_df[col] = train_df[col].astype(\"category\")\n",
    "\n",
    "\n",
    "train_dataset['USER_AGE'] = train_dataset['USER_AGE'].astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PURCHASE_VALUE']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######### FOLD 1 / 4 \n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's l1: 1.28146\tvalid_0's l2: 3.45995\n",
      "\n",
      "######### FOLD 2 / 4 \n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4]\tvalid_0's l1: 1.21003\tvalid_0's l2: 3.15118\n",
      "\n",
      "######### FOLD 3 / 4 \n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4]\tvalid_0's l1: 1.7957\tvalid_0's l2: 6.60986\n",
      "\n",
      "######### FOLD 4 / 4 \n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l1: 0.710979\tvalid_0's l2: 1.47227\n",
      " CV Mean Absolute Error :  1.2495428492044955\n",
      "(3616,)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "from tabnanny import verbose\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "X, y = train_dataset[features], train_dataset['USER_AGE']\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=123)\n",
    "feats = pd.DataFrame({'features': X.columns})\n",
    "gbm_predictions = []\n",
    "cv_score_ = 0\n",
    "oof_preds = np.zeros((train_dataset.shape[0],))\n",
    "for i, (tr_index, test_index) in enumerate(kf.split(X, y)):\n",
    "    print()\n",
    "    print(f'######### FOLD {i+1} / {kf.n_splits} ')\n",
    "    X_train, y_train = X.iloc[tr_index, :], y[tr_index]\n",
    "    X_test, y_test = X.iloc[test_index, :], y[test_index]\n",
    "\n",
    "    # lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    # lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "    # params = {'boosting_type': 'gbdt',\n",
    "    #           'objective': 'regression',\n",
    "    #           'max_depth': 8,\n",
    "    #           'num_leaves': 10,\n",
    "    #           'learning_rate': 0.5,\n",
    "    #           'feature_fraction': 0.9\n",
    "    #           }\n",
    "    # model = lgb.train(\n",
    "    #     params,\n",
    "    #     lgb_train,\n",
    "    #     num_boost_round=1000,\n",
    "    #     valid_sets=[lgb_train, lgb_eval],\n",
    "    #     valid_names=['train', 'valid'],\n",
    "    #     verbose_eval=0\n",
    "    # )\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "           learning_rate=0.6,\n",
    "           max_depth=5,\n",
    "        #    objective='multiclass',\n",
    "        #    boosting_type='dart',\n",
    "        num_leaves=3000,\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train,  eval_set=[\n",
    "        (X_test, y_test)], eval_metric='mae', early_stopping_rounds=200, verbose=200)\n",
    "\n",
    "    cv_score_ += mean_absolute_error(y_test, model.predict(X_test)) / kf.n_splits\n",
    "    oof_preds[test_index] = model.predict(X_test)\n",
    "    # oof_preds[test_index] = model.predict_proba(X_test)\n",
    "\n",
    "    preds = model.predict(train_df[X_train.columns])\n",
    "    gbm_predictions.append(preds)\n",
    "    # feats[f'Fold {i}'] = model.feature_importance()\n",
    "    feats[f'Fold {i}'] = model.feature_importances_\n",
    "\n",
    "feats['Importances'] = feats.mean(axis=1)\n",
    "print(' CV Mean Absolute Error : ', cv_score_)\n",
    "preds_xgb = np.average(gbm_predictions, axis=0)\n",
    "print(preds_xgb.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.249 = num_leaves = 2000, depth = 16, learning_rate = 0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df['predicted_age'] = preds_xgb\n",
    "test['predicted_age'] = preds_xgb\n",
    "\n",
    "# print(train_df.predicted_age.unique())\n",
    "\n",
    "#### Seeing how accurate the model is\n",
    "# train_df[train_df['USER_AGE']==32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  Specify the rows with imputted age \n",
    "\n",
    "generated_list_age =[]\n",
    "for age in train_df['USER_AGE']:\n",
    "    if age >=0:\n",
    "        generated_list_age.append(0)\n",
    "    else:\n",
    "        generated_list_age.append(1)\n",
    "\n",
    "train_df['generated_age']= generated_list_age\n",
    "\n",
    "\n",
    "generated_list_age =[]\n",
    "for age in test['USER_AGE']:\n",
    "    if age >=0:\n",
    "        generated_list_age.append(0)\n",
    "    else:\n",
    "        generated_list_age.append(1)\n",
    "\n",
    "test['generated_age']= generated_list_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>Fold 0</th>\n",
       "      <th>Fold 1</th>\n",
       "      <th>Fold 2</th>\n",
       "      <th>Fold 3</th>\n",
       "      <th>Importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PURCHASE_VALUE</td>\n",
       "      <td>99</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>51.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         features  Fold 0  Fold 1  Fold 2  Fold 3  Importances\n",
       "0  PURCHASE_VALUE      99       4       4     100        51.75"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df predicted ages value counts : 26.326504    1775\n",
      "27.925188    1139\n",
      "26.092356     344\n",
      "27.294236     304\n",
      "26.399787      54\n",
      "Name: predicted_age, dtype: int64\n",
      "extra data user age value counts : 25.0    404\n",
      "27.0    341\n",
      "26.0    172\n",
      "31.0    136\n",
      "32.0    104\n",
      "23.0      1\n",
      "Name: USER_AGE, dtype: int64\n",
      "train dataset user age value counts : 25.0    38\n",
      "27.0    14\n",
      "26.0     7\n",
      "32.0     2\n",
      "Name: USER_AGE, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"train_df predicted ages value counts :\", train_df.predicted_age.value_counts())\n",
    "\n",
    "print(\"extra data user age value counts :\", extra_data.USER_AGE.value_counts())\n",
    "\n",
    "print(\"train dataset user age value counts :\", train.USER_AGE.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "\n",
    "import pickle\n",
    "\n",
    "filename = 'age_prediction_model_1.249.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "# load the model from disk\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# result = loaded_model.score(X_test, Y_test)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(r\"C:\\Alvin/train_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USER_AGE  PURCHASE_VALUE\n",
       "26.0      150               17\n",
       "27.0      50                11\n",
       "          200               10\n",
       "26.0      200               10\n",
       "          100                9\n",
       "27.0      99                 9\n",
       "25.0      50                 9\n",
       "27.0      1050               8\n",
       "25.0      20                 7\n",
       "27.0      100                6\n",
       "25.0      1000               6\n",
       "27.0      40                 5\n",
       "26.0      500                5\n",
       "27.0      1000               5\n",
       "25.0      500                5\n",
       "          100                5\n",
       "          200                4\n",
       "27.0      2000               4\n",
       "          20                 4\n",
       "25.0      2000               3\n",
       "          490                3\n",
       "27.0      500                3\n",
       "25.0      700                3\n",
       "27.0      5                  3\n",
       "31.0      2900               3\n",
       "27.0      230                2\n",
       "25.0      1790               2\n",
       "27.0      130                2\n",
       "          48                 2\n",
       "          10                 2\n",
       "26.0      250                2\n",
       "27.0      1200               2\n",
       "          60                 2\n",
       "26.0      1000               2\n",
       "25.0      110                2\n",
       "27.0      50000              2\n",
       "25.0      300                2\n",
       "31.0      3000               2\n",
       "25.0      140                2\n",
       "27.0      1993               1\n",
       "Name: PURCHASE_VALUE, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 30))\n",
    "train_df_age.groupby(['USER_AGE', 'PURCHASE_VALUE'])[\n",
    "    'PURCHASE_VALUE'].count().sort_values(ascending=False).head(40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(r\"C:\\Alvin/test_augment.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "1a. Data Visualization.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "01e7be8b0026bafd93b41fb415e9db8b0da6a3f8cb0b48eca390d7b6a0c27d60"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
